#!/usr/bin/env python3
"""
Smart AI Engine v3 - Fully Offline AI-Driven
All responses generated by LOCAL AI model, no external API calls
Complete offline operation with local LLM inference
"""

import re
import json
import logging
import asyncio
import asyncpg
from typing import Dict, List, Optional, Tuple, Set, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, field
import os
from pathlib import Path
import time
from collections import defaultdict

# Import model registry for local model management
from .model_registry import ModelRegistry
# Import LLM-based search extractor for better search criteria extraction
from .llm_search_extractor import LLMSearchExtractor
# Import centralized prompt manager for all prompts
from .centralized_prompt_manager import CentralizedPromptManager
# Import model-driven intent service
from .model_driven_intent_service import ModelDrivenIntentService

logger = logging.getLogger(__name__)

@dataclass
class QueryContext:
    """Complete context for AI decision making"""
    message: str
    customer_id: str
    session_id: str
    customer_history: Optional[Dict] = None
    session_context: Optional[List] = None

@dataclass
class AIMetrics:
    """Comprehensive metrics for AI-only system monitoring"""
    total_llm_calls: int = 0
    successful_llm_calls: int = 0
    failed_llm_calls: int = 0
    total_response_time_ms: float = 0.0
    intent_detection_times: List[float] = field(default_factory=list)
    parameter_extraction_times: List[float] = field(default_factory=list)
    generation_times: List[float] = field(default_factory=list)
    errors_by_type: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    intents_detected: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    last_error: Optional[str] = None
    last_error_time: Optional[datetime] = None
    startup_time: datetime = field(default_factory=datetime.now)
    
    def get_average_response_time(self) -> float:
        """Get average response time in milliseconds"""
        if self.successful_llm_calls == 0:
            return 0.0
        return self.total_response_time_ms / self.successful_llm_calls
    
    def get_success_rate(self) -> float:
        """Get LLM call success rate as percentage"""
        if self.total_llm_calls == 0:
            return 0.0
        return (self.successful_llm_calls / self.total_llm_calls) * 100
    
    def log_metrics(self):
        """Log current metrics state"""
        logger.info(f"""
        === AI Engine Metrics (Pure LLM Mode) ===
        Uptime: {(datetime.now() - self.startup_time).total_seconds():.0f}s
        Total LLM Calls: {self.total_llm_calls}
        Success Rate: {self.get_success_rate():.1f}%
        Average Response Time: {self.get_average_response_time():.0f}ms
        Failed Calls: {self.failed_llm_calls}
        Last Error: {self.last_error} at {self.last_error_time}
        Top Intents: {dict(sorted(self.intents_detected.items(), key=lambda x: x[1], reverse=True)[:5])}
        ==========================================
        """)
    
class SmartAIEngineV3:
    """
    AI-driven engine for cannabis product recommendations
    No hardcoded responses - everything generated by the model
    Full conversation memory and learning capabilities
    """
    
    def __init__(self):
        # Local model management - no external APIs
        self.model_registry = ModelRegistry()
        self.llm = None
        self.model_config = None
        # Enhanced state tracking for memory and learning
        self.sessions = {}  # Session-based conversation history
        self.customer_profiles = {}  # Cross-session customer profiles
        self.learning_data = []  # Collected data for improvement
        
        # Cache for database configurations
        self._skip_words_cache: Set[str] = set()
        self._medical_intents_cache: Dict[str, List[str]] = {}
        self._category_keywords_cache: Dict[str, List[str]] = {}
        self._cache_timestamp = None
        self._cache_duration = timedelta(minutes=5)  # Refresh cache every 5 minutes
        
        # Database connection
        self.db_pool = None
        
        # Initialize LLM-based search extractor (will be set after LLM is loaded)
        self.search_extractor = None
        
        # Initialize model-driven intent service (will be set after LLM is loaded)
        self.intent_service = None
        
        # Initialize centralized prompt manager
        self.prompt_manager = CentralizedPromptManager()
        
        # Comprehensive metrics tracking for AI-only system
        self.metrics = AIMetrics()
        
        # Schedule periodic metrics logging
        self._metrics_log_interval = 60  # Log metrics every 60 seconds
        self._last_metrics_log = time.time()
        
    async def initialize(self):
        """Initialize AI engine with local model and database"""
        # Initialize database (optional for testing)
        if not self.db_pool:
            try:
                self.db_pool = await asyncpg.create_pool(
                    host=os.getenv('DB_HOST', 'localhost'),
                    port=int(os.getenv('DB_PORT', 5434)),
                    database=os.getenv('DB_NAME', 'ai_engine'),
                    user=os.getenv('DB_USER', 'weedgo'),
                    password=os.getenv('DB_PASSWORD', 'weedgo123')
                )
                await self._refresh_cache()
            except Exception as db_error:
                logger.warning(f"Database pool creation failed (running without DB): {db_error}")
                self.db_pool = None
        
        # Initialize model registry
        await self.model_registry.initialize_db()
        
        # Load local model (will use base model if no trained version exists)
        logger.info("Loading local AI model...")
        self.llm = await self.model_registry.load_model()
        
        if self.llm:
            self.model_config = self.model_registry.get_active_model_config()
            
            # Initialize the LLM-based search extractor with the loaded model
            self.search_extractor = LLMSearchExtractor(self.llm)
            logger.info("✅ LLM Search Extractor initialized")
            
            # Initialize the model-driven intent service with the loaded model
            self.intent_service = ModelDrivenIntentService(self.llm)
            logger.info("✅ Model-Driven Intent Service initialized")
            
            logger.info(f"✅ AI Engine initialized with model: {self.model_config.model_name if self.model_config else 'Base Model'}")
            return True
        else:
            error_msg = "❌ CRITICAL: Failed to load AI model. The AI engine cannot operate without an LLM."
            logger.error(error_msg)
            raise RuntimeError(error_msg)
    
    async def _refresh_cache(self):
        """Refresh cached configurations from database"""
        if not self.db_pool:
            await self.initialize_db()
            
        now = datetime.now()
        if self._cache_timestamp and (now - self._cache_timestamp) < self._cache_duration:
            return  # Cache is still fresh
        
        try:
            async with self.db_pool.acquire() as conn:
                # Load skip words
                skip_words_result = await conn.fetch(
                    "SELECT word FROM skip_words WHERE active = true"
                )
                self._skip_words_cache = {row['word'].lower() for row in skip_words_result}
                
                # Load medical intents (for now, use hardcoded until table structure is fixed)
                self._medical_intents_cache = {
                    "pain_relief": ["pain", "ache", "hurt", "sore", "discomfort"],
                    "anxiety_relief": ["anxiety", "anxious", "stress", "stressed", "nervous", "worry"],
                    "sleep_aid": ["sleep", "insomnia", "rest", "tired", "sleepless"],
                    "energy_boost": ["energy", "energetic", "active", "alert", "awake"],
                    "creativity": ["creative", "creativity", "artistic", "inspired"],
                    "appetite": ["appetite", "hungry", "munchies", "eat"],
                    "focus": ["focus", "concentrate", "attention", "productive"]
                }
                
                # Load actual categories from database
                categories_result = await conn.fetch(
                    """SELECT DISTINCT category, sub_category 
                       FROM products 
                       WHERE category IS NOT NULL 
                       ORDER BY category, sub_category"""
                )
                
                # Load sample of popular product names for model awareness
                products_sample = await conn.fetch(
                    """SELECT DISTINCT product_name, brand, street_name 
                       FROM products 
                       WHERE product_name IS NOT NULL 
                       LIMIT 100"""
                )
                
                # Build category structure from actual database
                self._category_keywords_cache = {}
                self._actual_categories = set()
                self._actual_subcategories = {}
                self._known_products = []  # Store sample product names
                
                for row in categories_result:
                    cat = row['category']
                    subcat = row['sub_category']
                    
                    self._actual_categories.add(cat)
                    
                    if cat not in self._actual_subcategories:
                        self._actual_subcategories[cat] = set()
                    if subcat:
                        self._actual_subcategories[cat].add(subcat)
                
                # Store product names for model awareness
                for row in products_sample:
                    if row['product_name']:
                        self._known_products.append(row['product_name'].lower())
                    
                    # Build keyword mappings based on actual categories
                    if cat not in self._category_keywords_cache:
                        self._category_keywords_cache[cat] = []
                    
                    # Add category-specific keywords
                    if cat == "Flower":
                        self._category_keywords_cache[cat] = ["flower", "bud", "nugs", "eighth", "quarter", "ounce", "dried"]
                    elif cat == "Edibles":
                        self._category_keywords_cache[cat] = ["edible", "gummy", "chocolate", "candy", "brownie", "cookie", "chews", "beverage"]
                    elif cat == "Vapes":
                        self._category_keywords_cache[cat] = ["vape", "cart", "cartridge", "pen", "disposable", "510", "thread"]
                    elif cat == "Extracts":
                        self._category_keywords_cache[cat] = ["dab", "shatter", "wax", "concentrate", "rosin", "resin", "distillate", "hash", "kief"]
                    elif cat == "Accessories":
                        self._category_keywords_cache[cat] = ["pipe", "bong", "grinder", "papers", "lighter"]
                    elif cat == "Topicals":
                        self._category_keywords_cache[cat] = ["topical", "cream", "lotion", "balm", "salve"]
                
                # Pre-Rolls is a subcategory, add special handling
                self._category_keywords_cache["Pre-Rolls"] = ["preroll", "pre-roll", "joint", "blunt", "j", "doobie"]
                
                logger.info(f"Loaded categories from database: {', '.join(sorted(self._actual_categories))}")
                logger.info(f"Total subcategories: {sum(len(subs) for subs in self._actual_subcategories.values())}")
                
                self._cache_timestamp = now
                logger.info(f"Configuration cache refreshed: {len(self._skip_words_cache)} skip words, "
                          f"{len(self._medical_intents_cache)} medical intents, "
                          f"{len(self._category_keywords_cache)} category configs")
                
        except Exception as e:
            logger.error(f"Error refreshing configuration cache: {e}")
            # Fall back to empty configurations if database is unavailable
            if not self._skip_words_cache:
                self._skip_words_cache = set()
            if not self._medical_intents_cache:
                self._medical_intents_cache = {}
            if not self._category_keywords_cache:
                self._category_keywords_cache = {}
    
    async def get_skip_words(self) -> Set[str]:
        """Get skip words from cache, refreshing if needed"""
        await self._refresh_cache()
        return self._skip_words_cache
    
    async def get_medical_intents(self) -> Dict[str, List[str]]:
        """Get medical intents from cache, refreshing if needed"""
        await self._refresh_cache()
        return self._medical_intents_cache
    
    async def get_category_keywords(self) -> Dict[str, List[str]]:
        """Get category keywords from cache, refreshing if needed"""
        await self._refresh_cache()
        return self._category_keywords_cache
        
    async def process_message(self, message: str, customer_id: str, session_id: str,
                            conversation_history: List[Dict] = None, 
                            budtender_personality: str = None,
                            customer_context: Dict = None) -> Dict:
        """
        Process message entirely through AI model - no fallbacks
        """
        
        # Track overall request timing
        request_start = time.time()
        
        # Ensure LLM is available before processing
        if not self.llm:
            self.metrics.errors_by_type['llm_not_available'] += 1
            self.metrics.last_error = "LLM not available at request time"
            self.metrics.last_error_time = datetime.now()
            return {
                "stage": "error",
                "message": "AI service is currently unavailable. Please try again later.",
                "products": [],
                "confidence": 0,
                "error": "LLM_NOT_AVAILABLE"
            }
        
        try:
            # Build context for the AI
            context = self._build_context(message, customer_id, session_id, conversation_history)
            
            # Store budtender personality and customer context
            context.budtender_personality = budtender_personality
            context.customer_context = customer_context or {}
            
            # Let AI determine the intent and response strategy
            intent = await self._determine_intent(context)
            
            # Generate appropriate response based on intent
            if intent.get('type') == 'greeting':
                result = await self._handle_greeting(context)
            elif intent.get('type') == 'product_search':
                result = await self._handle_product_search(context, intent)
            elif intent.get('type') == 'availability_check':
                # Treat availability check as product search with no specific criteria
                result = await self._handle_product_search(context, intent)
            elif intent.get('type') == 'cart_action':
                result = await self._handle_cart_action(context, intent)
            elif intent.get('type') == 'location':
                result = await self._handle_location_query(context)
            else:
                result = await self._handle_general_query(context)
            
            # Track successful response time
            elapsed_ms = (time.time() - request_start) * 1000
            self.metrics.total_response_time_ms += elapsed_ms
            
            # Log metrics periodically
            self._check_and_log_metrics()
            
            return result
            
        except Exception as e:
            # Track error
            self.metrics.errors_by_type[type(e).__name__] += 1
            self.metrics.last_error = str(e)
            self.metrics.last_error_time = datetime.now()
            logger.error(f"Message processing failed: {e}")
            
            # Return error response
            return {
                "stage": "error",
                "message": "An error occurred while processing your request. Please try again.",
                "products": [],
                "confidence": 0,
                "error": str(e)
            }
    
    def _build_context(self, message: str, customer_id: str, session_id: str, 
                      conversation_history: List[Dict] = None) -> QueryContext:
        """Build complete context for AI processing with full memory"""
        
        # Initialize session if new
        if session_id not in self.sessions:
            self.sessions[session_id] = {
                'messages': [],
                'context': {},
                'products_shown': [],
                'preferences': {}
            }
        
        # Add current message to session history
        self.sessions[session_id]['messages'].append({
            'role': 'user',
            'content': message,
            'timestamp': datetime.now().isoformat()
        })
        
        # Get or create customer profile
        if customer_id not in self.customer_profiles:
            self.customer_profiles[customer_id] = {
                'preferences': {},
                'purchase_history': [],
                'interaction_count': 0,
                'sessions': []
            }
        
        # Update customer profile
        self.customer_profiles[customer_id]['interaction_count'] += 1
        if session_id not in self.customer_profiles[customer_id]['sessions']:
            self.customer_profiles[customer_id]['sessions'].append(session_id)
        
        # Build comprehensive context
        return QueryContext(
            message=message,
            customer_id=customer_id,
            session_id=session_id,
            customer_history=self.customer_profiles[customer_id],
            session_context=self.sessions[session_id]['messages']
        )
    
    def _generate_local(self, prompt: str, max_tokens: int = 512) -> str:
        """
        Generate response using local LLM model
        Completely offline - no external API calls
        """
        if not self.llm:
            logger.error("No local model loaded")
            return ""
        
        try:
            # Use llama-cpp-python for local generation
            if hasattr(self.llm, '__call__'):
                # Real Llama model
                response = self.llm(
                    prompt,
                    max_tokens=max_tokens,
                    temperature=0.7,
                    top_p=0.95,
                    top_k=40,
                    repeat_penalty=1.1,
                    stop=["</s>", "[/INST]", "\n\n"]
                )
                
                if isinstance(response, dict) and 'choices' in response:
                    return response['choices'][0]['text'].strip()
                return str(response).strip()
            else:
                # Mock model for development
                return f"[Local response for: {prompt[:50]}...]"
                
        except Exception as e:
            logger.error(f"Local generation failed: {e}")
            return ""
    
    async def _determine_intent(self, context: QueryContext) -> Dict:
        """Determine intent using ONLY AI model - no pattern matching"""
        
        # Track timing
        start_time = time.time()
        
        # LLM is required - no fallbacks
        if not self.llm:
            self.metrics.errors_by_type['llm_not_available'] += 1
            self.metrics.last_error = "LLM not available"
            self.metrics.last_error_time = datetime.now()
            raise RuntimeError("AI model not available. Cannot process conversations without LLM.")
        
        try:
            # Build conversation context for better intent detection
            conversation_context = ""
            if context.session_context and len(context.session_context) > 0:
                recent = context.session_context[-3:]  # Last 3 messages for context
                for msg in recent:
                    role = "Customer" if msg.get('role') == 'user' else "Budtender"
                    conversation_context += f"{role}: {msg.get('content', '')}\n"
            
            # Use LLM for ALL intent detection - no pattern matching
            # Use centralized prompt manager
            prompt = self.prompt_manager.get_prompt(
                "intent_detection",
                conversation_context=f"Recent conversation:\n{conversation_context}" if conversation_context else "This is the start of a new conversation.",
                message=context.message
            )
            
            # Track LLM call
            self.metrics.total_llm_calls += 1
            response = self._generate_local(prompt, max_tokens=10)
            self.metrics.successful_llm_calls += 1
            
            response_lower = response.lower().strip()
            
            # Use model-driven intent detection
            if self.intent_service:
                # Get session context for intent detection
                session_context = []
                product_context = None
                
                if context.session_id in self.sessions:
                    session = self.sessions[context.session_id]
                    # Get recent conversation history
                    if 'conversation' in session:
                        session_context = session['conversation'][-5:]  # Last 5 messages
                    # Get product context if available
                    if session.get('last_products'):
                        product_context = {'name': session['last_products'][0].get('product_name', 'product')}
                
                # Detect intent using model
                intent_result = await self.intent_service.detect_intent(
                    text=context.message,
                    session_context=session_context,
                    product_context=product_context
                )
                
                # Track detected intent
                intent_type = intent_result.get('type', 'general')
                self.metrics.intents_detected[intent_type] += 1
                
                logger.info(f"Model-detected intent: {intent_type} (confidence: {intent_result.get('confidence', 0)})")
            else:
                # Fallback if intent service not available
                logger.warning("Intent service not available, using general intent")
                intent_result = {'type': 'general', 'confidence': 0.5}
                self.metrics.intents_detected['general_fallback'] += 1
            
            # Track timing
            elapsed_ms = (time.time() - start_time) * 1000
            self.metrics.intent_detection_times.append(elapsed_ms)
            
            # Log periodically
            self._check_and_log_metrics()
            
            return intent_result
            
        except Exception as e:
            self.metrics.failed_llm_calls += 1
            self.metrics.errors_by_type[type(e).__name__] += 1
            self.metrics.last_error = str(e)
            self.metrics.last_error_time = datetime.now()
            logger.error(f"Intent detection failed: {e}")
            raise
    
    def _check_and_log_metrics(self):
        """Check if it's time to log metrics and do so if needed"""
        current_time = time.time()
        if current_time - self._last_metrics_log >= self._metrics_log_interval:
            self.metrics.log_metrics()
            self._last_metrics_log = current_time
    
    async def _handle_greeting(self, context: QueryContext) -> Dict:
        """Handle greeting with AI-generated response using full context"""
        
        if not self.llm:
            # No fallback - LLM is required
            raise RuntimeError("AI model not available. Cannot generate responses without LLM.")
        
        # Build comprehensive context
        customer_info = []
        
        # Check customer history
        if context.customer_history:
            if context.customer_history.get('interaction_count', 0) > 1:
                customer_info.append(f"This is a returning customer (visit #{context.customer_history['interaction_count']})")
            
            if context.customer_history.get('preferences'):
                prefs = context.customer_history['preferences']
                if prefs.get('strain_type'):
                    customer_info.append(f"They prefer {prefs['strain_type']} strains")
                if prefs.get('category'):
                    customer_info.append(f"They usually buy {prefs['category']}")
        
        customer_context = '\n'.join(customer_info) if customer_info else "This is a new customer."
        
        # Get budtender personality from database data
        personality_guide = ""
        if hasattr(context, 'budtender_personality') and isinstance(context.budtender_personality, dict):
            # Full personality data from database
            p = context.budtender_personality
            name = p.get('name', 'Zac')
            age = p.get('age')
            gender = p.get('gender')
            style = p.get('communication_style')
            humor = p.get('humor_style')
            humor_level = p.get('humor_level')
            empathy = p.get('empathy_level')
            formality = p.get('formality')
            traits = p.get('traits', [])
            description = p.get('description')
            
            personality_guide = f"You are {name}"
            if age and gender:
                personality_guide += f", a {age} year old {gender} budtender"
            if description:
                personality_guide += f". {description}"
            if traits:
                personality_guide += f" Your key traits: {', '.join(traits)}."
            if humor and humor_level and humor_level != 'none':
                personality_guide += f" You have a {humor} sense of humor ({humor_level} level)."
            if style and formality and empathy:
                personality_guide += f" Communication style: {style}, formality: {formality}, empathy: {empathy}."
        else:
            # No personality data - use Zac as default
            personality_guide = "You are Zac, a friendly and knowledgeable cannabis consultant."
        
        # Add customer personalization
        if hasattr(context, 'customer_context') and context.customer_context:
            cc = context.customer_context
            if cc.get('customer_name') and cc['customer_name'] != 'Guest':
                personality_guide += f"\nThe customer's name is {cc['customer_name']}. Address them by name when appropriate."
            if cc.get('is_returning'):
                personality_guide += "\nThis is a returning customer, acknowledge their loyalty."
            if cc.get('preferred_strain'):
                personality_guide += f"\nThey prefer {cc['preferred_strain']} strains."
            if cc.get('favorite_products'):
                personality_guide += f"\nTheir favorite products include: {', '.join(cc['favorite_products'][:3])}"
        
        # Use centralized prompt manager
        prompt = self.prompt_manager.get_prompt(
            "greeting_response",
            personality=personality_guide,
            customer_context=customer_context,
            message=context.message
        )
        
        # Generate response using local model
        response_text = self._generate_local(prompt, max_tokens=100)
        
        if not response_text:
            response_text = "Welcome! How can I help you find the perfect cannabis products today?"
        
        return {
            "stage": "greeting",
            "message": response_text,
            "products": [],
            "quick_replies": [
                "Show me your best sellers",
                "I need something for pain relief",
                "What indica strains do you have?",
                "I'm new to cannabis"
            ],
            "confidence": 0.95 if response_text else 0.8
        }
    
    async def _handle_product_search(self, context: QueryContext, intent: Dict) -> Dict:
        """Handle product search with AI assistance and context awareness"""
        
        # ALWAYS SEARCH FIRST - Never assume products don't exist
        logger.info(f"Product search triggered for: {context.message}")
        
        # For availability checks, get diverse product selection
        if intent.get('type') == 'availability_check':
            # Get products from multiple categories for availability check
            products = await self._get_diverse_product_selection()
            search_params = {'showing_inventory': True}
        else:
            # Extract search parameters using BOTH LLM and keyword extraction
            search_params = await self._extract_search_params(context)
            
            # FALLBACK: If no params extracted, extract key terms directly from message
            if not any(search_params.values()):
                from services.search_first_engine import SearchFirstEngine
                temp_engine = SearchFirstEngine(self.db_pool)
                intent = temp_engine._extract_search_intent(context.message)
                search_params = {
                    'product': intent.product_name or '',
                    'category': intent.category or '',
                    'sub_category': intent.sub_category or '',
                    'size': intent.size or '',
                    'strain_type': intent.strain_type or ''
                }
                logger.info(f"Fallback extraction: {search_params}")
            
            # Check if this is a general product browsing request
            is_empty_search = all(not v for v in search_params.values())
            msg_lower = context.message.lower()
            general_requests = ['show me products', 'what do you have', 'show products', 'give me a product', 'give me product', 'give me products']
            
            # Also check if "what about" is followed by a product name
            if 'what about' in msg_lower and not is_empty_search:
                # This is a specific product inquiry, not general browsing
                pass
            elif is_empty_search and any(phrase in msg_lower for phrase in general_requests):
                # Get diverse product selection for browsing
                products = await self._get_diverse_product_selection()
                search_params = {'showing_inventory': True}
            else:
                # Enhance search with session preferences
                if context.session_id in self.sessions:
                    session_prefs = self.sessions[context.session_id].get('preferences', {})
                    # Merge session preferences with current search
                    for key, value in session_prefs.items():
                        if key not in search_params and value:
                            search_params[key] = value
                
                # Search for products
                products = await self._search_products(search_params)
        
        # If no products found, try searching individual words
        if not products and search_params.get('query'):
            words = search_params['query'].split()
            if len(words) > 1:
                logger.info(f"No results for '{search_params['query']}', trying individual words")
                # Try each word individually
                for word in words:
                    if len(word) > 2:  # Skip very short words
                        simple_params = {'query': word}
                        products = await self._search_products(simple_params)
                        if products:
                            logger.info(f"Found {len(products)} products for '{word}'")
                            search_params['query'] = word  # Update for message generation
                            break
        
        # Don't use fallback products - show actual search results only
        if not products:
            logger.info(f"No products found for search params: {search_params}")
        
        # Generate context-aware response
        message = await self._generate_search_response(context, products, search_params)
        
        # Store products and update session
        if context.session_id in self.sessions:
            self.sessions[context.session_id]['products_shown'].extend(products[:3])
            self.sessions[context.session_id]['last_products'] = products[:3]
            
            # Update preferences based on what was searched
            if search_params.get('strain_type'):
                self.sessions[context.session_id]['preferences']['strain_type'] = search_params['strain_type']
            if search_params.get('category'):
                self.sessions[context.session_id]['preferences']['category'] = search_params['category']
        
        # Add response to conversation history
        self.sessions[context.session_id]['messages'].append({
            'role': 'assistant',
            'content': message,
            'products': [p.get('product_name', '') for p in products[:3]],
            'timestamp': datetime.now().isoformat()
        })
        
        return {
            "stage": "smart_conversation",
            "message": message,
            "products": products[:3],
            "confidence": 0.9 if products else 0.5
        }
    
    async def _extract_search_params(self, context: QueryContext) -> Dict:
        """Extract search parameters using ONLY AI model - no fallbacks"""
        
        # LLM is required - no keyword matching fallbacks
        if not self.llm:
            raise RuntimeError("AI model not available. Cannot extract search parameters without LLM.")
        
        # Use the LLM Search Extractor for better extraction
        if self.search_extractor:
            try:
                # Extract criteria using the specialized extractor
                criteria = self.search_extractor.extract_search_criteria(context.message)
                
                # Convert to the format expected by the rest of the code
                search_params = {
                    "product": criteria.get("product_name", ""),
                    "size": criteria.get("size", ""),
                    "category": criteria.get("category", ""),
                    "strain_type": criteria.get("strain_type", ""),
                    "effects": "",  # Not extracted by default
                    "price_range": ""
                }
                
                # Add price range if present
                if criteria.get("max_price"):
                    search_params["price_range"] = f"under ${criteria['max_price']}"
                elif criteria.get("min_price"):
                    search_params["price_range"] = f"over ${criteria['min_price']}"
                
                logger.info(f"LLM Extractor returned: {search_params}")
                return search_params
                
            except Exception as e:
                logger.warning(f"LLM Extractor failed, falling back to original method: {e}")
                # Fall through to original method
        
        # Fallback to original extraction method
        if True:  # Keep structure for clarity
            # Build conversation context for AI
            conversation_text = self._format_conversation_for_ai(context)
            
            # Get actual categories from cache
            categories = sorted(getattr(self, '_actual_categories', ['Flower', 'Edibles', 'Vapes', 'Extracts']))
            categories_str = ', '.join(categories)
            
            # Include Topicals if it exists
            if 'Topicals' in categories:
                categories_str += ', Topicals'
            
            # Include sample of known products for context (helps with fuzzy matching)
            known_products = getattr(self, '_known_products', [])
            products_hint = ""
            if known_products:
                # Show a sample to give context without overwhelming
                sample = known_products[:20]
                products_hint = f"\n\nSAMPLE OF PRODUCTS WE CARRY (not exhaustive):\n{', '.join(sample)}"
            
            # Use centralized prompt manager for advanced search extraction
            prompt = self.prompt_manager.get_prompt(
                "advanced_search_extraction",
                categories_str=categories_str,
                products_hint=products_hint,
                conversation_text=conversation_text,
                message=context.message
            )
            
            try:
                # Generate using local model with reduced tokens to prevent verbose responses
                logger.info(f"Sending prompt to LLM for message: '{context.message}'")
                response_text = self._generate_local(prompt, max_tokens=100)
                logger.info(f"LLM response: {response_text[:200]}")  # Log first 200 chars
                
                # Clean response - find JSON even if model added text
                import json
                import re
                
                # Try to extract JSON from response
                cleaned_text = response_text
                
                # Remove common prefixes if they exist
                prefixes_to_remove = ['Example:', 'Response:', 'Here is', 'Example response:', '---', 'Output:']
                for prefix in prefixes_to_remove:
                    if cleaned_text.startswith(prefix):
                        cleaned_text = cleaned_text[len(prefix):].strip()
                
                # Try to find JSON object in the text
                json_match = re.search(r'\{[^{}]*\}', cleaned_text)
                if json_match:
                    cleaned_text = json_match.group(0)
                
                try:
                    extracted = json.loads(cleaned_text or '{}')
                    params = {k: v for k, v in extracted.items() if v}
                    logger.info(f"Message: '{context.message}' -> AI extracted params: {params}")
                    return params
                except json.JSONDecodeError:
                    logger.error(f"LLM returned invalid JSON even after cleanup. Original: {response_text}")
                    logger.error(f"Cleaned text: {cleaned_text}")
                    # No fallback - require proper JSON response
                    raise ValueError("AI model returned invalid JSON format for search parameters")
            except Exception as e:
                logger.error(f"AI extraction failed: {e}")
                raise RuntimeError(f"Failed to extract search parameters: {e}")
    
    async def _get_diverse_product_selection(self) -> List[Dict]:
        """Get a diverse selection of available products across categories"""
        
        if not self.db_pool:
            await self.initialize()
        
        try:
            async with self.db_pool.acquire() as conn:
                # Get 1-2 products from each major category
                query = """
                    WITH ranked_products AS (
                        SELECT 
                            p.*,
                            ROW_NUMBER() OVER (PARTITION BY p.category ORDER BY RANDOM()) as rn
                        FROM products p
                        WHERE p.category IN ('Flower', 'Edibles', 'Vapes', 'Extracts')
                          AND p.product_name IS NOT NULL
                          AND p.unit_price IS NOT NULL
                          AND p.unit_price > 0
                    )
                    SELECT * FROM ranked_products
                    WHERE rn <= 2
                    LIMIT 8
                """
                
                rows = await conn.fetch(query)
                
                products = []
                for row in rows:
                    product = {
                        "id": row['id'],
                        "product_name": row['product_name'],
                        "brand": row['brand'],
                        "category": row['category'],
                        "sub_category": row['sub_category'],
                        "thc_max": float(row['thc_max_percent'] or 0),
                        "cbd_max": float(row['cbd_max_percent'] or 0),
                        "unit_price": float(row['unit_price'] or 0),
                        "short_description": row['short_description'] or f"{row['category']} product from {row['brand'] or 'various brands'}",
                        "plant_type": row['plant_type'] or 'Hybrid',
                        "size": row['size'],
                        "unit_of_measure": row['unit_of_measure']
                    }
                    products.append(product)
                
                logger.info(f"Fetched {len(products)} diverse products for availability check")
                return products
                
        except Exception as e:
            logger.error(f"Error fetching diverse products: {e}")
            # Fallback to regular search
            return await self._search_products({'limit': 6})
    
    async def _search_products(self, params: Dict) -> List[Dict]:
        """Search for products in local database"""
        
        if not self.db_pool:
            await self.initialize()
        
        try:
            async with self.db_pool.acquire() as conn:
                # Build search query
                conditions = []
                db_params = []
                param_count = 0
                
                # Handle specific product name search (from extraction)
                if params.get('product'):
                    # Treat product name as primary search term
                    params['query'] = params['product']
                    params.pop('product', None)
                
                # Search by query/keywords with fuzzy matching
                if params.get('query'):
                    query_text = params['query'].lower()
                    
                    # Strategy 1: Exact substring match (catches most cases)
                    param_count += 1
                    db_params.append(f'%{query_text}%')
                    exact_condition = f"(LOWER(product_name) LIKE ${param_count} OR LOWER(brand) LIKE ${param_count} OR LOWER(street_name) LIKE ${param_count})"
                    
                    # Strategy 2: Split words for partial matches
                    word_conditions = []
                    words = query_text.split()
                    for word in words[:3]:  # Limit to 3 words
                        if len(word) > 2:  # Skip very short words
                            param_count += 1
                            db_params.append(f'%{word}%')
                            word_conditions.append(f"(LOWER(product_name) LIKE ${param_count} OR LOWER(brand) LIKE ${param_count})")
                    
                    # Combine strategies with OR
                    if word_conditions:
                        conditions.append(f"({exact_condition} OR {' OR '.join(word_conditions)})")
                    else:
                        conditions.append(exact_condition)
                
                # Search by category (handle Pre-Rolls specially)
                if params.get('category'):
                    param_count += 1
                    if params['category'] == 'Pre-Rolls':
                        db_params.append('Pre-Rolls')
                        conditions.append(f"sub_category = ${param_count}")
                    else:
                        db_params.append(params['category'])
                        conditions.append(f"category = ${param_count}")
                
                # Search by size (especially for joints/pre-rolls)
                if params.get('size'):
                    param_count += 1
                    size_value = params['size']
                    
                    # Handle common single-unit conversions
                    if size_value == '1g':
                        size_value = '1x1g'  # Database uses 1x1g format
                    elif size_value == '0.5g':
                        size_value = '1x0.5g'
                    elif size_value == '0.35g':
                        size_value = '1x0.35g'
                    
                    # Size should already be in XxYg format from LLM extractor
                    # But ensure consistency for any edge cases
                    db_params.append(size_value)
                    conditions.append(f"size = ${param_count}")
                
                # Search by strain type (using plant_type field)
                # Handle variations like 'Sativa', 'Sativa Dominant', etc.
                if params.get('strain_type'):
                    param_count += 1
                    # Use ILIKE for case-insensitive partial matching
                    db_params.append(f"%{params['strain_type']}%")
                    conditions.append(f"plant_type ILIKE ${param_count}")
                
                # Build final query
                where_clause = " AND ".join(conditions) if conditions else "1=1"
                
                query = f"""
                    SELECT id, product_name, brand, category, sub_category,
                           street_name, plant_type, size, thc_max_percent, cbd_max_percent, 
                           unit_price, short_description, long_description, image_url,
                           supplier_name
                    FROM products
                    WHERE {where_clause}
                    ORDER BY 
                        CASE WHEN unit_price IS NOT NULL AND unit_price > 0 THEN unit_price ELSE 999999 END ASC
                    LIMIT 20
                """
                
                results = await conn.fetch(query, *db_params)
                
                products = []
                for row in results:
                    products.append({
                        'id': row['id'],
                        'product_name': row['product_name'],
                        'brand': row['brand'],
                        'category': row['category'],
                        'sub_category': row.get('sub_category', ''),
                        'size': row.get('size', ''),
                        'plant_type': row.get('plant_type', ''),
                        'strain_type': row.get('plant_type', ''),
                        'thc': row.get('thc_max_percent', 0),
                        'cbd': row.get('cbd_max_percent', 0),
                        'price': float(row['unit_price']) if row.get('unit_price') else 0,
                        'description': row.get('short_description', ''),
                        'image': row.get('image_url', '')
                    })
                
                logger.info(f"Local database search returned {len(products)} products")
                return products
                    
        except Exception as e:
            logger.error(f"Local product search failed: {e}")
        
        return []
    
    def _format_conversation_for_ai(self, context: QueryContext) -> str:
        """Format conversation history for AI understanding"""
        
        if not context.session_context:
            return "No previous conversation"
        
        # Format last 5 messages for context
        recent_messages = context.session_context[-5:] if len(context.session_context) > 5 else context.session_context
        
        formatted = []
        for msg in recent_messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            if role == 'user':
                formatted.append(f"Customer: {content}")
            else:
                formatted.append(f"Budtender: {content}")
                # Include products shown if any
                if msg.get('products'):
                    formatted.append(f"  (Showed: {', '.join(msg['products'])})")
        
        return '\n'.join(formatted)
    
    async def _generate_search_response(self, context: QueryContext, 
                                      products: List[Dict], 
                                      search_params: Dict) -> str:
        """Generate natural response for search results using ONLY AI"""
        
        # RAG Implementation: ALWAYS use actual search results
        logger.info(f"Generating response for {len(products)} products found")
        
        if not self.llm:
            # Fallback to factual response without LLM
            if products:
                return f"Yes! We have {products[0].get('product_name', 'that product')} for ${products[0].get('price', 0):.2f}. Would you like to add it to your cart?"
            else:
                return "I searched our inventory but couldn't find that specific item. Would you like to see what else we have?"
        
        # Use AI to generate natural response with full context
        product_summary = ""
        if products:
            product_details = []
            for p in products[:5]:  # Show more products
                details = f"✓ {p.get('product_name', 'Unknown')} (${p.get('price', 0):.2f})"
                if p.get('size'):
                    details += f" - {p.get('size')}"
                if p.get('category'):
                    details += f" [{p.get('category')}/{p.get('sub_category', '')}]"
                product_details.append(details)
            product_summary = f"FOUND {len(products)} PRODUCTS:\n" + '\n'.join(product_details)
        else:
            product_summary = "SEARCH RESULT: No products found matching your request"
        
        # Include conversation context
        conversation_text = self._format_conversation_for_ai(context)
        
        # Get budtender personality from database data
        personality_guide = ""
        if hasattr(context, 'budtender_personality') and isinstance(context.budtender_personality, dict):
            # Full personality data from database
            p = context.budtender_personality
            name = p.get('name', 'Zac')
            style = p.get('communication_style')
            humor = p.get('humor_style')
            humor_level = p.get('humor_level')
            sales_approach = p.get('sales_approach')
            traits = p.get('traits', [])
            
            personality_guide = f"You are {name}"
            if style:
                personality_guide += f", a {style} budtender"
            if sales_approach:
                personality_guide += f" with a {sales_approach} sales approach"
            if humor and humor_level and humor_level != 'none':
                personality_guide += f", using {humor} humor"
            if traits:
                personality_guide += f". Key traits: {', '.join(traits)}"
        else:
            # No personality data - use Zac as default
            personality_guide = "You are Zac, a friendly and knowledgeable cannabis consultant."
            
        # Prepare products context
        products_context = f"ACTUAL SEARCH RESULTS:\n{product_summary}\nTotal found: {len(products)} products"
        
        # If showing inventory, add subcategory list
        if search_params.get('showing_inventory'):
            # Get unique subcategories from the products
            subcategories = set()
            for p in products:
                if p.get('sub_category'):
                    subcategories.add(p['sub_category'])
            
            if subcategories:
                subcategory_list = ', '.join(sorted(subcategories))
                products_context += f"\n\nAvailable subcategories: {subcategory_list}"
        
        # Use appropriate prompt based on context
        if search_params.get('showing_inventory'):
            # Use inventory display prompt for "what do you have" type questions
            prompt = self.prompt_manager.get_prompt(
                "inventory_display",
                personality=personality_guide,
                message=context.message,
                products_context=products_context
            )
        else:
            # Use standard product search response
            prompt = self.prompt_manager.get_prompt(
                "product_search_response",
                personality=personality_guide,
                conversation_text=conversation_text,
                message=context.message,
                search_performed=f"Searched for: {search_params}",
                products_context=products_context,
                result_count=str(len(products))
            )
        
        # Generate using local model
        response_text = self._generate_local(prompt, max_tokens=80)
        
        if not response_text:
            raise RuntimeError("AI model failed to generate response")
            
        return response_text
    
    async def _handle_cart_action(self, context: QueryContext, intent: Dict) -> Dict:
        """Handle cart/purchase actions"""
        
        # Check for products in session
        session_data = self.sessions.get(context.session_id, {})
        last_products = session_data.get('last_products', [])
        
        if not last_products:
            return {
                "stage": "smart_conversation",
                "message": "Could you first tell me which product you're interested in?",
                "products": [],
                "confidence": 0.7
            }
        
        # Determine which product they want
        selected_product = last_products[0]  # Default to first
        
        # Generate confirmation message
        if self.llm:
            # Use centralized prompt manager
            prompt = self.prompt_manager.get_prompt(
                "cart_confirmation",
                product_name=selected_product.get('product_name', 'that item')
            )
            
            # Generate using local model
            response_text = self._generate_local(prompt, max_tokens=40)
            
            if response_text:
                message = response_text
            else:
                message = f"Great choice! I've added {selected_product.get('product_name', 'that')} to your cart."
        else:
            message = f"Great choice! I've added {selected_product.get('product_name', 'that')} to your cart."
        
        return {
            "stage": "smart_conversation",
            "message": message,
            "products": [selected_product],
            "confidence": 1.0
        }
    
    async def _handle_location_query(self, context: QueryContext) -> Dict:
        """Handle location and store information queries"""
        
        if not self.llm:
            # Provide store location directly without LLM
            return {
                "stage": "smart_conversation",
                "message": "We're located at 553 Rogers Road, Toronto. Hours: Mon-Fri 10am-10pm, Sat 10am-11pm, Sun 11am-8pm.",
                "products": [],
                "confidence": 1.0
            }
        
        # Use centralized prompt manager for location inquiry
        prompt = self.prompt_manager.get_prompt(
            "location_inquiry",
            message=context.message
        )
        
        # Generate using local model
        response_text = self._generate_local(prompt, max_tokens=50)
        
        if response_text:
            return {
                "stage": "smart_conversation",
                "message": response_text,
                "products": [],
                "confidence": 1.0
            }
        else:
            # Fallback to hardcoded location
            return {
                "stage": "smart_conversation",
                "message": "We're at 553 Rogers Road, Toronto. Open Mon-Fri 10am-10pm, Sat 10am-11pm, Sun 11am-8pm.",
                "products": [],
                "confidence": 1.0
            }
    
    async def _handle_general_query(self, context: QueryContext) -> Dict:
        """Handle general queries with AI"""
        
        if not self.llm:
            return {
                "stage": "smart_conversation",
                "message": "I'm here to help you find cannabis products. What are you looking for?",
                "products": [],
                "confidence": 0.5
            }
        
        # Use centralized prompt manager
        prompt = self.prompt_manager.get_prompt(
            "error_recovery",
            message=context.message
        )
        
        # Generate using local model
        response_text = self._generate_local(prompt, max_tokens=100)
        
        if response_text:
            return {
                "stage": "smart_conversation",
                "message": response_text,
                "products": [],
                "confidence": 0.8
            }
        else:
            return {
                "stage": "smart_conversation",
                "message": "How can I help you find the right products today?",
                "products": [],
                "confidence": 0.5
            }

# Create a global instance
smart_ai_engine_v3 = SmartAIEngineV3()